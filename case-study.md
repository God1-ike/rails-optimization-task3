# Case-study оптимизации

## Актуальные проблемы
В нашем проекте возникло несколько серьёзных проблем.
1) Долгий и не оптимальный импорт данных, при при увеличении объема
2) Страница Отображение расписаний, начи нает тормозить при увеличении данных.

## Импорт данных
В приложении есть rake таска, которая удаляет все ранее загруженные данные, и добавляет новые из выбранного файла.
```
bin/rake reload_json[file]
```
Возникла проблема что с увеличением данных программа выполнятеся долго.
Я решил исправить эту проблему, оптимизировав эту программу.
### Формирование метрики
Для того, чтобы понимать, дают ли мои изменения положительный эффект на быстродействие программы я придумал использовать такую метрику, программа должна выполнять файл large.json меньше чем за 60 сек.

Предворительно фиксируем что 
- small.json: "Result Time 21.576042000000598"
- medium.json: "Result Time 125.41547200000787"

### Гарантия корректности работы оптимизированной программы
Выполнение этого теста в фидбек-лупе позволяет не допустить изменения логики программы при оптимизации.

Так же добавился тест чтобы проверить лоигку работы и скорость работы на разных объемах данных


### Feedback-Loop
Для того, чтобы иметь возможность быстро проверять гипотезы я выстроил эффективный `feedback-loop`, который позволил мне получать обратную связь по эффективности сделанных изменений.

Вот как я построил `feedback_loop`: 
1) запустил программа на разных кол-вах данных
2) проверил логи, чтобы отследить проблемные места
3) внес изменения
4) прогнал тесты
5) зафиксировал результат


### Ваша находка
- запустил импорт exmaple.json файла и начал смотреть логи
- по логами и логике видно что оснавное время тратится на лишние взаимодейстия с базой Select или сразу Insert.
- Оптимальным решением будет собрать данные сперва и потом импортировать их через active-storage-import, если не хочется тянуть гем то можно и чесез insert_all
  + так же пришлось для удобсва создать можель на связь BusesService
  + все объекты сохранял сразу в переменную и hash 
    ```
    to = cities[trip['to']] ||= City.new(name: trip['to'])
    ```
    1) в переменную которая использутся в других обектах
    2) аггрегатор который собиарет всё вместе
    PS. Если не использовать одиночную переменную, а вытаскивать из агрегатора(везде это Hash), то тратится много времени на поиск объекта(если использовать только Hash)
    ```
    "1 0.75399"
    "2 20.596119"
    "3 20.603491"
    "4 20.698374"
    "5 20.700828"
    "6 76.098364"
    "7 99.592368"
    "Result Time 99.59396600000036"
    ``` 
   
- как результат я уложился в поставленную задачу, весь импорт large файла занимает 29.5 сек
```
"1 0.954356"
"2 7.650278"
"3 7.654027"
"4 7.734918"
"5 7.752177"
"6 8.211075"
"7 29.495458"
"Result Time 29.50383200000215"
```